#!/usr/bin/env python3
"""
Callback Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ LiteLLM Router.
"""

from __future__ import annotations

import logging
import threading
from typing import Any, Dict, List

logger = logging.getLogger(__name__)

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð°Ñ Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²ÐºÐ° Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
_update_lock = threading.Lock()


def update_models_in_router(models: List[Dict[str, Any]]) -> None:
    """
    ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² LiteLLM Router Ñ‡ÐµÑ€ÐµÐ· upsert_deployment.
    
    Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² Ñ‚Ð¾Ð¼ Ð¶Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐµ, Ñ‡Ñ‚Ð¾ Ð¸ LiteLLM ÑÐµÑ€Ð²ÐµÑ€,
    Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ ÑÑ€Ð°Ð·Ñƒ Ð²Ð¸Ð´Ð½Ñ‹ Ð² /v1/models.
    
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²ÐºÑƒ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ race conditions.
    """
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²ÐºÑƒ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
    with _update_lock:
        try:
            import litellm.proxy.proxy_server as proxy_server
            from litellm.types.router import Deployment, LiteLLM_Params, ModelInfo
            import os
            
            logger.info(f"âš¡ Ð—Ð°Ð¿ÑƒÑ‰ÐµÐ½ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ {len(models)} Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² LiteLLM...")

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Router ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚
            if not hasattr(proxy_server, "llm_router") or proxy_server.llm_router is None:
                logger.warning("âš ï¸ Router ÐµÑ‰Ñ‘ Ð½Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ")
                return
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÑƒÑ„Ñ„Ð¸ÐºÑ Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
            model_suffix = os.environ.get("PROXY_PROVIDER_MODEL_SUFFIX", "proxy")
            sync_suffix = f"-{model_suffix}"
            
            # 1. Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð’Ð¡Ð• ÑÑ‚Ð°Ñ€Ñ‹Ðµ deployments Ñ Ð½Ð°ÑˆÐ¸Ð¼ ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ¾Ð¼
            # Ð­Ñ‚Ð¾ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ Ð¿Ñ€Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸ upsert_deployment
            deleted_count = 0
            if hasattr(proxy_server.llm_router, 'model_list'):
                original_count = len(proxy_server.llm_router.model_list)
                
                # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ - Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð‘Ð•Ð— Ð½Ð°ÑˆÐµÐ³Ð¾ ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ°
                proxy_server.llm_router.model_list = [
                    d for d in proxy_server.llm_router.model_list 
                    if not d.get("model_name", "").endswith(sync_suffix)
                ]
                
                deleted_count = original_count - len(proxy_server.llm_router.model_list)
                if deleted_count > 0:
                    logger.info(f"ðŸ—‘ï¸  Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ {deleted_count} ÑÑ‚Ð°Ñ€Ñ‹Ñ… deployments Ñ ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ¾Ð¼ {sync_suffix}")
            
            # 2. Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼/Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¸Ð· API
            logger.info(f"ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ {len(models)} Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð² Router...")
            
            added_count = 0
            updated_count = 0
            
            for model in models:
                try:
                    model_name = model.get("model_name")
                    litellm_params_dict = model.get("litellm_params", {})
                    
                    if not model_name or not litellm_params_dict:
                        logger.warning(f"âš ï¸ ÐŸÑ€Ð¾Ð¿ÑƒÑÐº Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð½ÐµÐ¿Ð¾Ð»Ð½Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸: {model}")
                        continue
                    
                    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ LiteLLM_Params
                    litellm_params = LiteLLM_Params(
                        model=litellm_params_dict.get("model"),
                        api_base=litellm_params_dict.get("api_base"),
                        api_key=litellm_params_dict.get("api_key", "none"),
                        timeout=litellm_params_dict.get("timeout"),
                    )
                    
                    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Deployment
                    deployment = Deployment(
                        model_name=model_name,
                        litellm_params=litellm_params,
                        model_info=ModelInfo()
                    )
                    
                    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼/Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² Router
                    result = proxy_server.llm_router.upsert_deployment(deployment)
                    
                    if result is not None:
                        added_count += 1
                        logger.debug(f"  âœ“ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð°: {model_name}")
                    else:
                        updated_count += 1
                        logger.debug(f"  â†» ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð°: {model_name}")
                        
                except Exception as model_exc:
                    logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ {model.get('model_name')}: {model_exc}")
                    continue
            
            logger.info(f"âœ… Router Ð¾Ð±Ð½Ð¾Ð²Ð»Ñ‘Ð½: Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ {added_count}, Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ {updated_count}, ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {deleted_count} Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹")
            
            # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
            current_models = proxy_server.llm_router.get_model_names()
            logger.info(f"ðŸ“‹ Ð¢ÐµÐºÑƒÑ‰Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð² Router ({len(current_models)}):")
            for model_name in current_models:
                logger.info(f"  - {model_name}")

        except Exception as exc:
            logger.error(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ: {exc}", exc_info=True)


def get_update_callback() -> callable:
    return update_models_in_router
